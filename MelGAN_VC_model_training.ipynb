{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MelGAN_VC_model_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wojciechsadlik/MelGAN-VC-ThesisExperiments/blob/master/MelGAN_VC_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We'll be using TF 2.1 and torchaudio\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "#!pip install soundfile                    #to save wav files\n",
        "#!pip install --no-deps torchaudio==0.5.0"
      ],
      "metadata": {
        "id": "9JV6E4j1Ko4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAmiyxtl2J5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f173de93-767f-44da-835f-0baf9144ef30"
      },
      "source": [
        "#Connecting Drive to save model checkpoints during training and to use custom data, uncomment if needed\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEvqwT96l_Yq"
      },
      "source": [
        "from IPython.core.display import JSON\n",
        "#Imports\n",
        "\n",
        "from __future__ import print_function, division\n",
        "from glob import glob\n",
        "import scipy\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Concatenate, Conv2D, Conv2DTranspose, GlobalAveragePooling2D, UpSampling2D, LeakyReLU, ReLU, Add, Multiply, Lambda, Dot, BatchNormalization, Activation, ZeroPadding2D, Cropping2D, Cropping1D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import TruncatedNormal, he_normal\n",
        "import tensorflow.keras.backend as K\n",
        "import datetime\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "import imageio\n",
        "import librosa\n",
        "import librosa.display\n",
        "from librosa.feature import melspectrogram\n",
        "import os\n",
        "import time\n",
        "import IPython\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbaM4WKrvO7r"
      },
      "source": [
        "#Hyperparameters\n",
        "\n",
        "hop=192               #hop size (window size = 6*hop)\n",
        "sr=16000              #sampling rate\n",
        "min_level_db=-100     #reference values to normalize data\n",
        "ref_level_db=20\n",
        "\n",
        "shape=24              #length of time axis of split specrograms to feed to generator            \n",
        "vec_len=128           #length of vector generated by siamese vector\n",
        "bs = 16               #batch size\n",
        "delta = 2.            #constant for siamese loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_BASE_PATH = '/content/drive/MyDrive/GTZAN_dataset'\n",
        "\n",
        "SPEC_TYPE = 'genres_melspectrograms'\n",
        "\n",
        "A_GENRE = 'jazz'                                               #source\n",
        "B_GENRE = 'classical'                                          #target\n",
        "\n",
        "MODEL_SAVE_PATH = os.path.join('/content/drive/MyDrive/GTZAN_dataset', SPEC_TYPE, f'{A_GENRE}-{B_GENRE}(id)')"
      ],
      "metadata": {
        "id": "yX_LZCBFPR07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNRYjsCDqDjF"
      },
      "source": [
        "#Helper functions\n",
        "\n",
        "#Split spectrograms in chunks with equal size\n",
        "def splitcut(data):\n",
        "  ls = []\n",
        "  mini = 0\n",
        "  minifinal = 10*shape                                                              #max spectrogram length\n",
        "  for i in range(data.shape[0]-1):\n",
        "    if data[i].shape[1]<=data[i+1].shape[1]:\n",
        "      mini = data[i].shape[1]\n",
        "    else:\n",
        "      mini = data[i+1].shape[1]\n",
        "    if mini>=3*shape and mini<minifinal:\n",
        "      minifinal = mini\n",
        "  for i in range(data.shape[0]):\n",
        "    x = data[i]\n",
        "    if x.shape[1]>=3*shape:\n",
        "      for n in range(x.shape[1]//minifinal):\n",
        "        ls.append(x[:,n*minifinal:n*minifinal+minifinal,:])\n",
        "      ls.append(x[:,-minifinal:,:])\n",
        "  return np.array(ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_spectrograms(dir_path):\n",
        "  ls = glob(f'{dir_path}/*.npy')\n",
        "\n",
        "  spectrograms = np.empty(len(ls), dtype=object)\n",
        "  \n",
        "  for i in range(len(ls)):\n",
        "    spectrograms[i] = np.load(ls[i])\n",
        "\n",
        "  return spectrograms"
      ],
      "metadata": {
        "id": "x2_Y3gCb-FXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK_UnhfMELHD"
      },
      "source": [
        "#Loading spectrogram dataset (Uncomment where needed)\n",
        "#adata: source spectrograms\n",
        "#bdata: target spectrograms\n",
        "\n",
        "aspec = load_spectrograms(os.path.join(DATASET_BASE_PATH, SPEC_TYPE, A_GENRE))   #get spectrogram array\n",
        "adata = splitcut(aspec)                                                         #split spectrogams to fixed length\n",
        "\n",
        "bspec = load_spectrograms(os.path.join(DATASET_BASE_PATH, SPEC_TYPE, B_GENRE))\n",
        "bdata = splitcut(bspec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSesIbwr_GyO"
      },
      "source": [
        "#Creating Tensorflow Datasets\n",
        "\n",
        "@tf.function\n",
        "def proc(x):\n",
        "  return tf.image.random_crop(x, size=[hop, 3*shape, 1])\n",
        "\n",
        "dsa = tf.data.Dataset.from_tensor_slices(adata).repeat(50).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)\n",
        "dsb = tf.data.Dataset.from_tensor_slices(bdata).repeat(50).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHnP2zr7Ypgi"
      },
      "source": [
        "#Adding Spectral Normalization to convolutional layers\n",
        "\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import sparse_ops\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.python.ops import standard_ops\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (tf.norm(v) + eps)\n",
        "\n",
        "\n",
        "class ConvSN2D(tf.keras.layers.Conv2D):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
        "        super(ConvSN2D, self).__init__(filters, kernel_size, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ConvSN2D, self).build(input_shape)\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
        "            initializer=tf.initializers.RandomNormal(0, 1),\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        for _ in range(self.power_iterations):\n",
        "\n",
        "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "            new_u = l2normalize(tf.matmul(new_v, W))\n",
        "            \n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W/sigma\n",
        "\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "          W_bar = tf.reshape(W_bar, W_shape)\n",
        "\n",
        "        return W_bar\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "        outputs = self.convolution_op(inputs, new_kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "            if self.data_format == 'channels_first':\n",
        "                    outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
        "            else:\n",
        "                outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class ConvSN2DTranspose(tf.keras.layers.Conv2DTranspose):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
        "        super(ConvSN2DTranspose, self).__init__(filters, kernel_size, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ConvSN2DTranspose, self).build(input_shape)\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
        "            initializer=tf.initializers.RandomNormal(0, 1),\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        for _ in range(self.power_iterations):\n",
        "\n",
        "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "            new_u = l2normalize(tf.matmul(new_v, W))\n",
        "            \n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W/sigma\n",
        "\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "          W_bar = tf.reshape(W_bar, W_shape)\n",
        "\n",
        "        return W_bar\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "\n",
        "        inputs_shape = array_ops.shape(inputs)\n",
        "        batch_size = inputs_shape[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "          h_axis, w_axis = 2, 3\n",
        "        else:\n",
        "          h_axis, w_axis = 1, 2\n",
        "\n",
        "        height, width = inputs_shape[h_axis], inputs_shape[w_axis]\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.strides\n",
        "\n",
        "        if self.output_padding is None:\n",
        "          out_pad_h = out_pad_w = None\n",
        "        else:\n",
        "          out_pad_h, out_pad_w = self.output_padding\n",
        "\n",
        "        out_height = conv_utils.deconv_output_length(height,\n",
        "                                                    kernel_h,\n",
        "                                                    padding=self.padding,\n",
        "                                                    output_padding=out_pad_h,\n",
        "                                                    stride=stride_h,\n",
        "                                                    dilation=self.dilation_rate[0])\n",
        "        out_width = conv_utils.deconv_output_length(width,\n",
        "                                                    kernel_w,\n",
        "                                                    padding=self.padding,\n",
        "                                                    output_padding=out_pad_w,\n",
        "                                                    stride=stride_w,\n",
        "                                                    dilation=self.dilation_rate[1])\n",
        "        if self.data_format == 'channels_first':\n",
        "          output_shape = (batch_size, self.filters, out_height, out_width)\n",
        "        else:\n",
        "          output_shape = (batch_size, out_height, out_width, self.filters)\n",
        "\n",
        "        output_shape_tensor = array_ops.stack(output_shape)\n",
        "        outputs = K.conv2d_transpose(\n",
        "            inputs,\n",
        "            new_kernel,\n",
        "            output_shape_tensor,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if not context.executing_eagerly():\n",
        "          out_shape = self.compute_output_shape(inputs.shape)\n",
        "          outputs.set_shape(out_shape)\n",
        "\n",
        "        if self.use_bias:\n",
        "          outputs = tf.nn.bias_add(\n",
        "              outputs,\n",
        "              self.bias,\n",
        "              data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n",
        "\n",
        "        if self.activation is not None:\n",
        "          return self.activation(outputs)\n",
        "        return outputs  \n",
        "    \n",
        "    \n",
        "class DenseSN(Dense):\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        super(DenseSN, self).build(input_shape)\n",
        "\n",
        "        self.u = self.add_weight(self.name + '_u',\n",
        "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
        "            initializer=tf.initializers.RandomNormal(0, 1),\n",
        "            trainable=False)\n",
        "        \n",
        "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
        "        new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
        "        new_u = l2normalize(tf.matmul(new_v, W))\n",
        "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
        "        W_bar = W/sigma\n",
        "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
        "          W_bar = tf.reshape(W_bar, W_shape)\n",
        "        return W_bar\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
        "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
        "        rank = len(inputs.shape)\n",
        "        if rank > 2:\n",
        "          outputs = standard_ops.tensordot(inputs, new_kernel, [[rank - 1], [0]])\n",
        "          if not context.executing_eagerly():\n",
        "            shape = inputs.shape.as_list()\n",
        "            output_shape = shape[:-1] + [self.units]\n",
        "            outputs.set_shape(output_shape)\n",
        "        else:\n",
        "          inputs = math_ops.cast(inputs, self._compute_dtype)\n",
        "          if K.is_sparse(inputs):\n",
        "            outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, new_kernel)\n",
        "          else:\n",
        "            outputs = gen_math_ops.mat_mul(inputs, new_kernel)\n",
        "        if self.use_bias:\n",
        "          outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "          return self.activation(outputs)\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX41awYeHE1N"
      },
      "source": [
        "#Networks Architecture\n",
        "\n",
        "init = tf.keras.initializers.he_uniform()\n",
        "\n",
        "def conv2d(layer_input, filters, kernel_size=4, strides=2, padding='same', leaky=True, bnorm=True, sn=True):\n",
        "  if leaky:\n",
        "    Activ = LeakyReLU(alpha=0.2)\n",
        "  else:\n",
        "    Activ = ReLU()\n",
        "  if sn:\n",
        "    d = ConvSN2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
        "  else:\n",
        "    d = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
        "  if bnorm:\n",
        "    d = BatchNormalization()(d)\n",
        "  d = Activ(d)\n",
        "  return d\n",
        "\n",
        "def deconv2d(layer_input, layer_res, filters, kernel_size=4, conc=True, scalev=False, bnorm=True, up=True, padding='same', strides=2):\n",
        "  if up:\n",
        "    u = UpSampling2D((1,2))(layer_input)\n",
        "    u = ConvSN2D(filters, kernel_size, strides=(1,1), kernel_initializer=init, use_bias=False, padding=padding)(u)\n",
        "  else:\n",
        "    u = ConvSN2DTranspose(filters, kernel_size, strides=strides, kernel_initializer=init, use_bias=False, padding=padding)(layer_input)\n",
        "  if bnorm:\n",
        "    u = BatchNormalization()(u)\n",
        "  u = LeakyReLU(alpha=0.2)(u)\n",
        "  if conc:\n",
        "    u = Concatenate()([u,layer_res])\n",
        "  return u\n",
        "\n",
        "#Extract function: splitting spectrograms\n",
        "def extract_image(im):\n",
        "  im1 = Cropping2D(((0,0), (0, 2*(im.shape[2]//3))))(im)\n",
        "  im2 = Cropping2D(((0,0), (im.shape[2]//3,im.shape[2]//3)))(im)\n",
        "  im3 = Cropping2D(((0,0), (2*(im.shape[2]//3), 0)))(im)\n",
        "  return im1,im2,im3\n",
        "\n",
        "#Assemble function: concatenating spectrograms\n",
        "def assemble_image(lsim):\n",
        "  im1,im2,im3 = lsim\n",
        "  imh = Concatenate(2)([im1,im2,im3])\n",
        "  return imh\n",
        "\n",
        "#U-NET style architecture\n",
        "def build_generator(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=input_shape)\n",
        "  #downscaling\n",
        "  g0 = tf.keras.layers.ZeroPadding2D((0,1))(inp)\n",
        "  g1 = conv2d(g0, 256, kernel_size=(h,3), strides=1, padding='valid')\n",
        "  g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2))\n",
        "  g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  #upscaling\n",
        "  g4 = deconv2d(g3,g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  g5 = deconv2d(g4,g1, 256, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
        "  g6 = ConvSN2DTranspose(1, kernel_size=(h,1), strides=(1,1), kernel_initializer=init, padding='valid', activation='tanh')(g5)\n",
        "  return Model(inp,g6, name='G')\n",
        "\n",
        "#Siamese Network\n",
        "def build_siamese(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=input_shape)\n",
        "  g1 = conv2d(inp, 256, kernel_size=(h,3), strides=1, padding='valid', sn=False)\n",
        "  g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2), sn=False)\n",
        "  g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2), sn=False)\n",
        "  g4 = Flatten()(g3)\n",
        "  g5 = Dense(vec_len)(g4)\n",
        "  return Model(inp, g5, name='S')\n",
        "\n",
        "#Discriminator (Critic) Network\n",
        "def build_critic(input_shape):\n",
        "  h,w,c = input_shape\n",
        "  inp = Input(shape=input_shape)\n",
        "  g1 = conv2d(inp, 512, kernel_size=(h,3), strides=1, padding='valid', bnorm=False)\n",
        "  g2 = conv2d(g1, 512, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
        "  g3 = conv2d(g2, 512, kernel_size=(1,7), strides=(1,2), bnorm=False)\n",
        "  g4 = Flatten()(g3)\n",
        "  g4 = DenseSN(1, kernel_initializer=init)(g4)\n",
        "  return Model(inp, g4, name='C')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fXJmItOzrhC"
      },
      "source": [
        "#Load past models from path to resume training or test\n",
        "def load(path):\n",
        "  gen = build_generator((hop,shape,1))\n",
        "  siam = build_siamese((hop,shape,1))\n",
        "  critic = build_critic((hop,3*shape,1))\n",
        "  gen.load_weights(path+'/gen.h5')\n",
        "  critic.load_weights(path+'/critic.h5')\n",
        "  siam.load_weights(path+'/siam.h5')\n",
        "  return gen,critic,siam\n",
        "\n",
        "#Build models\n",
        "def build():\n",
        "  gen = build_generator((hop,shape,1))\n",
        "  siam = build_siamese((hop,shape,1))\n",
        "  critic = build_critic((hop,3*shape,1))                                          #the discriminator accepts as input spectrograms of triple the width of those generated by the generator\n",
        "  return gen,critic,siam\n",
        "\n",
        "#Save in training loop\n",
        "def save_end(epoch,gloss,closs,idloss,e_time,n_save=3,save_path='../content/'):                 #use custom save_path (i.e. Drive '../content/drive/My Drive/')\n",
        "  if epoch % n_save == 0:\n",
        "    print('Saving...')\n",
        "    path = f'{save_path}/MELGANVC-{epoch}'\n",
        "    os.makedirs(path)\n",
        "    gen.save_weights(path+'/gen.h5')\n",
        "    critic.save_weights(path+'/critic.h5')\n",
        "    siam.save_weights(path+'/siam.h5')\n",
        "    \n",
        "    with open(os.path.join(path, 'losses.json'), 'w') as losses_file:\n",
        "      json.dump({'epoch': str(epoch),\n",
        "                 'g_loss': str(gloss)[:9],\n",
        "                 'c_loss': str(closs)[:9],\n",
        "                 'id_loss': str(idloss)[:9],\n",
        "                 'e_time': str(e_time)[:9]},\n",
        "                losses_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn2s65AxjDJ8"
      },
      "source": [
        "#Losses\n",
        "\n",
        "def mae(x,y):\n",
        "  return tf.reduce_mean(tf.abs(x-y))\n",
        "\n",
        "def mse(x,y):\n",
        "  return tf.reduce_mean((x-y)**2)\n",
        "\n",
        "def loss_travel(sa,sab,sa1,sab1):\n",
        "  l1 = tf.reduce_mean(((sa-sa1) - (sab-sab1))**2)\n",
        "  l2 = tf.reduce_mean(tf.reduce_sum(-(tf.nn.l2_normalize(sa-sa1, axis=[-1]) * tf.nn.l2_normalize(sab-sab1, axis=[-1])), axis=-1))\n",
        "  return l1+l2\n",
        "\n",
        "def loss_siamese(sa,sa1):\n",
        "  logits = tf.sqrt(tf.reduce_sum((sa-sa1)**2, axis=-1, keepdims=True))\n",
        "  return tf.reduce_mean(tf.square(tf.maximum((delta - logits), 0)))\n",
        "\n",
        "def d_loss_f(fake):\n",
        "  return tf.reduce_mean(tf.maximum(1 + fake, 0))\n",
        "\n",
        "def d_loss_r(real):\n",
        "  return tf.reduce_mean(tf.maximum(1 - real, 0))\n",
        "\n",
        "def g_loss_f(fake):\n",
        "  return tf.reduce_mean(- fake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgjxHjyIhPwl"
      },
      "source": [
        "#Get models and optimizers\n",
        "def get_networks(shape, load_model=False, path=None):\n",
        "  if not load_model:\n",
        "    gen,critic,siam = build()\n",
        "  else:\n",
        "    gen,critic,siam = load(path)\n",
        "  print('Built networks')\n",
        "\n",
        "  opt_gen = Adam(0.0001, 0.5)\n",
        "  opt_disc = Adam(0.0001, 0.5)\n",
        "\n",
        "  return gen,critic,siam, [opt_gen,opt_disc]\n",
        "\n",
        "#Set learning rate\n",
        "def update_lr(lr):\n",
        "  opt_gen.learning_rate = lr\n",
        "  opt_disc.learning_rate = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGWjgHqDWR78"
      },
      "source": [
        "#Training Functions\n",
        "\n",
        "#Train Generator, Siamese and Critic\n",
        "@tf.function\n",
        "def train_all(a,b):\n",
        "  #splitting spectrogram in 3 parts\n",
        "  aa,aa2,aa3 = extract_image(a) \n",
        "  bb,bb2,bb3 = extract_image(b)\n",
        "\n",
        "  with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:\n",
        "\n",
        "    #translating A to B\n",
        "    fab = gen(aa, training=True)\n",
        "    fab2 = gen(aa2, training=True)\n",
        "    fab3 = gen(aa3, training=True)\n",
        "    #identity mapping B to B                                                        COMMENT THESE 3 LINES IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
        "    fid = gen(bb, training=True) \n",
        "    fid2 = gen(bb2, training=True)\n",
        "    fid3 = gen(bb3, training=True)\n",
        "    #concatenate/assemble converted spectrograms\n",
        "    fabtot = assemble_image([fab,fab2,fab3])\n",
        "\n",
        "    #feed concatenated spectrograms to critic\n",
        "    cab = critic(fabtot, training=True)\n",
        "    cb = critic(b, training=True)\n",
        "    #feed 2 pairs (A,G(A)) extracted spectrograms to Siamese\n",
        "    sab = siam(fab, training=True)\n",
        "    sab2 = siam(fab3, training=True)\n",
        "    sa = siam(aa, training=True)\n",
        "    sa2 = siam(aa3, training=True)\n",
        "\n",
        "    #identity mapping loss\n",
        "    #loss_id = 0\n",
        "    loss_id = (mae(bb,fid)+mae(bb2,fid2)+mae(bb3,fid3))/3.                         #loss_id = 0. IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
        "    #travel loss\n",
        "    loss_m = loss_travel(sa,sab,sa2,sab2)+loss_siamese(sa,sa2)\n",
        "    #generator and critic losses\n",
        "    loss_g = g_loss_f(cab)\n",
        "    loss_dr = d_loss_r(cb)\n",
        "    loss_df = d_loss_f(cab)\n",
        "    loss_d = (loss_dr+loss_df)/2.\n",
        "    #generator+siamese total loss\n",
        "    lossgtot = loss_g+10.*loss_m+0.5*loss_id                                       #CHANGE LOSS WEIGHTS HERE  (COMMENT OUT +w*loss_id IF THE IDENTITY LOSS TERM IS NOT NEEDED)\n",
        "  \n",
        "  #computing and applying gradients\n",
        "  grad_gen = tape_gen.gradient(lossgtot, gen.trainable_variables+siam.trainable_variables)\n",
        "  opt_gen.apply_gradients(zip(grad_gen, gen.trainable_variables+siam.trainable_variables))\n",
        "\n",
        "  grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
        "  opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
        "  \n",
        "  return loss_dr,loss_df,loss_g,loss_id\n",
        "\n",
        "#Train Critic only\n",
        "@tf.function\n",
        "def train_d(a,b):\n",
        "  aa,aa2,aa3 = extract_image(a)\n",
        "  with tf.GradientTape() as tape_disc:\n",
        "\n",
        "    fab = gen(aa, training=True)\n",
        "    fab2 = gen(aa2, training=True)\n",
        "    fab3 = gen(aa3, training=True)\n",
        "    fabtot = assemble_image([fab,fab2,fab3])\n",
        "\n",
        "    cab = critic(fabtot, training=True)\n",
        "    cb = critic(b, training=True)\n",
        "\n",
        "    loss_dr = d_loss_r(cb)\n",
        "    loss_df = d_loss_f(cab)\n",
        "\n",
        "    loss_d = (loss_dr+loss_df)/2.\n",
        "  \n",
        "  grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
        "  opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
        "\n",
        "  return loss_dr,loss_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVwL-Ry-nNru"
      },
      "source": [
        "#Training Loop\n",
        "\n",
        "def train(epochs, batch_size=16, lr=0.0001, n_save=6, gupt=5, epoch_start=0):\n",
        "  \n",
        "  update_lr(lr)\n",
        "  df_list = []\n",
        "  dr_list = []\n",
        "  g_list = []\n",
        "  id_list = []\n",
        "  c = 0\n",
        "  g = 0\n",
        "  \n",
        "  for epoch in range(epoch_start, epochs):\n",
        "        bef = time.time()\n",
        "        \n",
        "        for batchi,(a,b) in enumerate(zip(dsa,dsb)):\n",
        "          \n",
        "            if batchi%gupt==0:\n",
        "              dloss_t,dloss_f,gloss,idloss = train_all(a,b)\n",
        "            else:\n",
        "              dloss_t,dloss_f = train_d(a,b)\n",
        "\n",
        "            df_list.append(dloss_f)\n",
        "            dr_list.append(dloss_t)\n",
        "            g_list.append(gloss)\n",
        "            id_list.append(idloss)\n",
        "            c += 1\n",
        "            g += 1\n",
        "\n",
        "            if batchi%600==0:\n",
        "                print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis=0)} ', end='')\n",
        "                print(f'r: {np.mean(dr_list[-g:], axis=0)}] ', end='')\n",
        "                print(f'[G loss: {np.mean(g_list[-g:], axis=0)}] ', end='')\n",
        "                print(f'[ID loss: {np.mean(id_list[-g:])}] ', end='')\n",
        "                print(f'[LR: {lr}]')\n",
        "                g = 0\n",
        "            nbatch=batchi\n",
        "\n",
        "        e_time = time.time()-bef\n",
        "        print(f'Time/Batch {e_time/nbatch}')\n",
        "        print(f'Time/Epoch {e_time}')\n",
        "        print(f'Memory {tf.config.experimental.get_memory_info(\"GPU:0\")}')\n",
        "        save_end(epoch,np.mean(g_list[-n_save*c:], axis=0),\n",
        "                 np.mean(df_list[-n_save*c:], axis=0),\n",
        "                 np.mean(id_list[-n_save*c:], axis=0),\n",
        "                 e_time,\n",
        "                 n_save=n_save,save_path=MODEL_SAVE_PATH)\n",
        "        print(f'Mean D loss: {np.mean(df_list[-c:], axis=0)} Mean G loss: {np.mean(g_list[-c:], axis=0)} Mean ID loss: {np.mean(id_list[-c:], axis=0)}')\n",
        "        c = 0\n",
        "                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JruweKNrl_ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a2a63d-8cc3-4b02-dbfc-24ad66663862"
      },
      "source": [
        "#Build models and initialize optimizers\n",
        "\n",
        "#If load_model=True, specify the path where the models are saved\n",
        "\n",
        "#gen,critic,siam, [opt_gen,opt_disc] = get_networks(shape, load_model=False, path='../content/drive/My Drive/GTZAN_dataset')\n",
        "gen,critic,siam, [opt_gen,opt_disc] = get_networks(shape, load_model=True, path=os.path.join(MODEL_SAVE_PATH, 'MELGANVC-130'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built networks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BknKCA-8yqap",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd05a05-5cc3-4d68-ab7f-2a82cd84613b"
      },
      "source": [
        "#Training\n",
        "\n",
        "#n_save = how many epochs between each saving and displaying of results\n",
        "#gupt = how many discriminator updates for generator+siamese update\n",
        "\n",
        "train(1000, batch_size=bs, lr=0.0002, n_save=5, gupt=3, epoch_start=131)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 131/1000] [Batch 0] [D loss f: 0.08676182478666306 r: 1.7617084980010986] [G loss: 1.1248269081115723] [ID loss: 0.1745506376028061] [LR: 0.0002]\n",
            "[Epoch 131/1000] [Batch 600] [D loss f: 0.3134636878967285 r: 0.428194522857666] [G loss: 0.9971408843994141] [ID loss: 0.16334500908851624] [LR: 0.0002]\n",
            "[Epoch 131/1000] [Batch 1200] [D loss f: 0.4046597182750702 r: 0.5171579718589783] [G loss: 0.8251107931137085] [ID loss: 0.15663152933120728] [LR: 0.0002]\n",
            "[Epoch 131/1000] [Batch 1800] [D loss f: 0.39341506361961365 r: 0.5195464491844177] [G loss: 0.8621019721031189] [ID loss: 0.15775597095489502] [LR: 0.0002]\n",
            "[Epoch 131/1000] [Batch 2400] [D loss f: 0.3789558708667755 r: 0.49511921405792236] [G loss: 0.8383633494377136] [ID loss: 0.15500293672084808] [LR: 0.0002]\n",
            "[Epoch 131/1000] [Batch 3000] [D loss f: 0.37470850348472595 r: 0.4949975609779358] [G loss: 0.8383049368858337] [ID loss: 0.1561773419380188] [LR: 0.0002]\n",
            "Time/Batch 0.12324817631960897\n",
            "Memory {'current': 656433920, 'peak': 5224965120}\n",
            "Mean D loss: 0.3725934624671936 Mean G loss: 0.8700248599052429 Mean ID loss: 0.1573556661605835\n",
            "[Epoch 132/1000] [Batch 0] [D loss f: 0.36984214186668396 r: 0.48479339480400085] [G loss: 0.8533597588539124] [ID loss: 0.15412308275699615] [LR: 0.0002]\n",
            "[Epoch 132/1000] [Batch 600] [D loss f: 0.3789115250110626 r: 0.4863893687725067] [G loss: 0.8351370096206665] [ID loss: 0.1543404459953308] [LR: 0.0002]\n",
            "[Epoch 132/1000] [Batch 1200] [D loss f: 0.36363154649734497 r: 0.4738418161869049] [G loss: 0.852231502532959] [ID loss: 0.1514866203069687] [LR: 0.0002]\n",
            "[Epoch 132/1000] [Batch 1800] [D loss f: 0.3615078628063202 r: 0.47681310772895813] [G loss: 0.8469118475914001] [ID loss: 0.15176032483577728] [LR: 0.0002]\n",
            "[Epoch 132/1000] [Batch 2400] [D loss f: 0.37148794531822205 r: 0.48480600118637085] [G loss: 0.8373298048973083] [ID loss: 0.15175434947013855] [LR: 0.0002]\n",
            "[Epoch 132/1000] [Batch 3000] [D loss f: 0.36986950039863586 r: 0.47463342547416687] [G loss: 0.840671718120575] [ID loss: 0.1507824957370758] [LR: 0.0002]\n",
            "Time/Batch 0.11270625418876634\n",
            "Memory {'current': 658756864, 'peak': 5224965120}\n",
            "Mean D loss: 0.3671870827674866 Mean G loss: 0.8439493775367737 Mean ID loss: 0.1519635170698166\n",
            "[Epoch 133/1000] [Batch 0] [D loss f: 0.35313597321510315 r: 0.453519344329834] [G loss: 0.8545517325401306] [ID loss: 0.15150250494480133] [LR: 0.0002]\n",
            "[Epoch 133/1000] [Batch 600] [D loss f: 0.3548685312271118 r: 0.46715518832206726] [G loss: 0.8460549116134644] [ID loss: 0.15020990371704102] [LR: 0.0002]\n",
            "[Epoch 133/1000] [Batch 1200] [D loss f: 0.3470717966556549 r: 0.46213099360466003] [G loss: 0.853700578212738] [ID loss: 0.15205635130405426] [LR: 0.0002]\n",
            "[Epoch 133/1000] [Batch 1800] [D loss f: 0.35013434290885925 r: 0.4623887240886688] [G loss: 0.854992151260376] [ID loss: 0.15015943348407745] [LR: 0.0002]\n",
            "[Epoch 133/1000] [Batch 2400] [D loss f: 0.35312971472740173 r: 0.4541860520839691] [G loss: 0.837521493434906] [ID loss: 0.14960333704948425] [LR: 0.0002]\n",
            "[Epoch 133/1000] [Batch 3000] [D loss f: 0.33573880791664124 r: 0.45160767436027527] [G loss: 0.8486124277114868] [ID loss: 0.1479390263557434] [LR: 0.0002]\n",
            "Time/Batch 0.11274961938863638\n",
            "Memory {'current': 661079552, 'peak': 5224965120}\n",
            "Mean D loss: 0.34875741600990295 Mean G loss: 0.8439396619796753 Mean ID loss: 0.14962396025657654\n",
            "[Epoch 134/1000] [Batch 0] [D loss f: 0.35338279604911804 r: 0.4653327763080597] [G loss: 0.8118132948875427] [ID loss: 0.1468779593706131] [LR: 0.0002]\n",
            "[Epoch 134/1000] [Batch 600] [D loss f: 0.3439371883869171 r: 0.4554702341556549] [G loss: 0.8340531587600708] [ID loss: 0.1481599658727646] [LR: 0.0002]\n",
            "[Epoch 134/1000] [Batch 1200] [D loss f: 0.3415088951587677 r: 0.4407159388065338] [G loss: 0.8276021480560303] [ID loss: 0.14805208146572113] [LR: 0.0002]\n",
            "[Epoch 134/1000] [Batch 1800] [D loss f: 0.35090234875679016 r: 0.4481727182865143] [G loss: 0.8749793767929077] [ID loss: 0.14810998737812042] [LR: 0.0002]\n",
            "[Epoch 134/1000] [Batch 2400] [D loss f: 0.34192097187042236 r: 0.4576002359390259] [G loss: 0.8449153304100037] [ID loss: 0.14717262983322144] [LR: 0.0002]\n",
            "[Epoch 134/1000] [Batch 3000] [D loss f: 0.3500066101551056 r: 0.4471774399280548] [G loss: 0.8309760689735413] [ID loss: 0.1478162407875061] [LR: 0.0002]\n",
            "Time/Batch 0.1126389818146676\n",
            "Memory {'current': 663404288, 'peak': 5224965120}\n",
            "Mean D loss: 0.3455236256122589 Mean G loss: 0.8414725065231323 Mean ID loss: 0.14793619513511658\n",
            "[Epoch 135/1000] [Batch 0] [D loss f: 0.3441075086593628 r: 0.4525352418422699] [G loss: 0.8344049453735352] [ID loss: 0.1484881490468979] [LR: 0.0002]\n",
            "[Epoch 135/1000] [Batch 600] [D loss f: 0.33012762665748596 r: 0.4404759705066681] [G loss: 0.8572729229927063] [ID loss: 0.14715467393398285] [LR: 0.0002]\n",
            "[Epoch 135/1000] [Batch 1200] [D loss f: 0.3341163694858551 r: 0.44888296723365784] [G loss: 0.8529806137084961] [ID loss: 0.14745981991291046] [LR: 0.0002]\n",
            "[Epoch 135/1000] [Batch 1800] [D loss f: 0.3413848280906677 r: 0.4456600844860077] [G loss: 0.850131630897522] [ID loss: 0.14698074758052826] [LR: 0.0002]\n",
            "[Epoch 135/1000] [Batch 2400] [D loss f: 0.3374584913253784 r: 0.44558075070381165] [G loss: 0.8377843499183655] [ID loss: 0.146611288189888] [LR: 0.0002]\n",
            "[Epoch 135/1000] [Batch 3000] [D loss f: 0.3317200839519501 r: 0.450437992811203] [G loss: 0.8221637010574341] [ID loss: 0.14688147604465485] [LR: 0.0002]\n",
            "Time/Batch 0.11264064520825225\n",
            "Memory {'current': 665728256, 'peak': 5224965120}\n",
            "Saving...\n",
            "Mean D loss: 0.3351020812988281 Mean G loss: 0.840074360370636 Mean ID loss: 0.14704802632331848\n",
            "[Epoch 136/1000] [Batch 0] [D loss f: 0.3359575569629669 r: 0.44721242785453796] [G loss: 0.8106852769851685] [ID loss: 0.14723123610019684] [LR: 0.0002]\n",
            "[Epoch 136/1000] [Batch 600] [D loss f: 0.3324272036552429 r: 0.4514775574207306] [G loss: 0.8861262202262878] [ID loss: 0.1459602415561676] [LR: 0.0002]\n",
            "[Epoch 136/1000] [Batch 1200] [D loss f: 0.33611321449279785 r: 0.4535908102989197] [G loss: 0.855588972568512] [ID loss: 0.14465810358524323] [LR: 0.0002]\n",
            "[Epoch 136/1000] [Batch 1800] [D loss f: 0.3284599184989929 r: 0.4442007541656494] [G loss: 0.8596699833869934] [ID loss: 0.14527493715286255] [LR: 0.0002]\n",
            "[Epoch 136/1000] [Batch 2400] [D loss f: 0.33288589119911194 r: 0.44430792331695557] [G loss: 0.8489108681678772] [ID loss: 0.14523693919181824] [LR: 0.0002]\n",
            "[Epoch 136/1000] [Batch 3000] [D loss f: 0.3350345492362976 r: 0.434604287147522] [G loss: 0.8256979584693909] [ID loss: 0.1452392339706421] [LR: 0.0002]\n",
            "Time/Batch 0.11274202552562178\n",
            "Memory {'current': 668050432, 'peak': 5224965120}\n",
            "Mean D loss: 0.33322766423225403 Mean G loss: 0.8533845543861389 Mean ID loss: 0.14536982774734497\n",
            "[Epoch 137/1000] [Batch 0] [D loss f: 0.3354831039905548 r: 0.43123555183410645] [G loss: 0.8393262624740601] [ID loss: 0.14614005386829376] [LR: 0.0002]\n",
            "[Epoch 137/1000] [Batch 600] [D loss f: 0.32985740900039673 r: 0.4400045871734619] [G loss: 0.8702369928359985] [ID loss: 0.14713770151138306] [LR: 0.0002]\n",
            "[Epoch 137/1000] [Batch 1200] [D loss f: 0.3380223512649536 r: 0.4418395459651947] [G loss: 0.8468089699745178] [ID loss: 0.14260278642177582] [LR: 0.0002]\n",
            "[Epoch 137/1000] [Batch 1800] [D loss f: 0.3318561315536499 r: 0.4377886950969696] [G loss: 0.8382724523544312] [ID loss: 0.144855797290802] [LR: 0.0002]\n",
            "[Epoch 137/1000] [Batch 2400] [D loss f: 0.33053722977638245 r: 0.44051504135131836] [G loss: 0.8366177082061768] [ID loss: 0.14513297379016876] [LR: 0.0002]\n",
            "[Epoch 137/1000] [Batch 3000] [D loss f: 0.32676422595977783 r: 0.4337359070777893] [G loss: 0.8271113038063049] [ID loss: 0.14504297077655792] [LR: 0.0002]\n",
            "Time/Batch 0.1126487418387232\n",
            "Memory {'current': 670374656, 'peak': 5224965120}\n",
            "Mean D loss: 0.33123335242271423 Mean G loss: 0.8467233180999756 Mean ID loss: 0.14486798644065857\n",
            "[Epoch 138/1000] [Batch 0] [D loss f: 0.329778254032135 r: 0.4328201413154602] [G loss: 0.8685786128044128] [ID loss: 0.14424331486225128] [LR: 0.0002]\n",
            "[Epoch 138/1000] [Batch 600] [D loss f: 0.33271998167037964 r: 0.44199275970458984] [G loss: 0.835632860660553] [ID loss: 0.14525765180587769] [LR: 0.0002]\n",
            "[Epoch 138/1000] [Batch 1200] [D loss f: 0.3280978798866272 r: 0.43562331795692444] [G loss: 0.8535334467887878] [ID loss: 0.14330309629440308] [LR: 0.0002]\n",
            "[Epoch 138/1000] [Batch 1800] [D loss f: 0.3248802125453949 r: 0.43405354022979736] [G loss: 0.8518076539039612] [ID loss: 0.14178578555583954] [LR: 0.0002]\n",
            "[Epoch 138/1000] [Batch 2400] [D loss f: 0.3290719985961914 r: 0.4311918616294861] [G loss: 0.8267307281494141] [ID loss: 0.14270760118961334] [LR: 0.0002]\n",
            "[Epoch 138/1000] [Batch 3000] [D loss f: 0.32440078258514404 r: 0.4370877146720886] [G loss: 0.8268016576766968] [ID loss: 0.14411400258541107] [LR: 0.0002]\n",
            "Time/Batch 0.11261506756216831\n",
            "Memory {'current': 672699392, 'peak': 5224965120}\n",
            "Mean D loss: 0.327817440032959 Mean G loss: 0.8394728302955627 Mean ID loss: 0.14342127740383148\n",
            "[Epoch 139/1000] [Batch 0] [D loss f: 0.3280158042907715 r: 0.43312758207321167] [G loss: 0.8432278037071228] [ID loss: 0.1432546079158783] [LR: 0.0002]\n",
            "[Epoch 139/1000] [Batch 600] [D loss f: 0.3262850046157837 r: 0.43565434217453003] [G loss: 0.8525315523147583] [ID loss: 0.14242354035377502] [LR: 0.0002]\n",
            "[Epoch 139/1000] [Batch 1200] [D loss f: 0.3227076232433319 r: 0.43138858675956726] [G loss: 0.8443330526351929] [ID loss: 0.14329656958580017] [LR: 0.0002]\n",
            "[Epoch 139/1000] [Batch 1800] [D loss f: 0.31803807616233826 r: 0.426562637090683] [G loss: 0.8432271480560303] [ID loss: 0.1422213762998581] [LR: 0.0002]\n",
            "[Epoch 139/1000] [Batch 2400] [D loss f: 0.3174300491809845 r: 0.434846431016922] [G loss: 0.8809316158294678] [ID loss: 0.14062437415122986] [LR: 0.0002]\n",
            "[Epoch 139/1000] [Batch 3000] [D loss f: 0.3272683918476105 r: 0.4325532913208008] [G loss: 0.8410615921020508] [ID loss: 0.14129851758480072] [LR: 0.0002]\n",
            "Time/Batch 0.11264207319677892\n",
            "Memory {'current': 675020032, 'peak': 5224965120}\n",
            "Mean D loss: 0.32200929522514343 Mean G loss: 0.8511740565299988 Mean ID loss: 0.14184893667697906\n",
            "[Epoch 140/1000] [Batch 0] [D loss f: 0.319419264793396 r: 0.4340936839580536] [G loss: 0.8420875072479248] [ID loss: 0.14096882939338684] [LR: 0.0002]\n",
            "[Epoch 140/1000] [Batch 600] [D loss f: 0.31798267364501953 r: 0.42660507559776306] [G loss: 0.8690431714057922] [ID loss: 0.14165997505187988] [LR: 0.0002]\n",
            "[Epoch 140/1000] [Batch 1200] [D loss f: 0.32274582982063293 r: 0.42778196930885315] [G loss: 0.8381326794624329] [ID loss: 0.14277437329292297] [LR: 0.0002]\n",
            "[Epoch 140/1000] [Batch 1800] [D loss f: 0.31471380591392517 r: 0.42712849378585815] [G loss: 0.8501150608062744] [ID loss: 0.14289605617523193] [LR: 0.0002]\n",
            "[Epoch 140/1000] [Batch 2400] [D loss f: 0.31909582018852234 r: 0.4334535300731659] [G loss: 0.8363915085792542] [ID loss: 0.1414642333984375] [LR: 0.0002]\n",
            "[Epoch 140/1000] [Batch 3000] [D loss f: 0.32570788264274597 r: 0.4363851845264435] [G loss: 0.8610960841178894] [ID loss: 0.14268040657043457] [LR: 0.0002]\n",
            "Time/Batch 0.11264595818617706\n",
            "Memory {'current': 677345024, 'peak': 5224965120}\n",
            "Saving...\n",
            "Mean D loss: 0.31976819038391113 Mean G loss: 0.8529181480407715 Mean ID loss: 0.1422494649887085\n",
            "[Epoch 141/1000] [Batch 0] [D loss f: 0.31719133257865906 r: 0.42604807019233704] [G loss: 0.8681766390800476] [ID loss: 0.14187948405742645] [LR: 0.0002]\n",
            "[Epoch 141/1000] [Batch 600] [D loss f: 0.32081273198127747 r: 0.42641207575798035] [G loss: 0.8514146208763123] [ID loss: 0.14041468501091003] [LR: 0.0002]\n",
            "[Epoch 141/1000] [Batch 1200] [D loss f: 0.3223503530025482 r: 0.42488592863082886] [G loss: 0.8533337116241455] [ID loss: 0.14178314805030823] [LR: 0.0002]\n",
            "[Epoch 141/1000] [Batch 1800] [D loss f: 0.3138609826564789 r: 0.42460429668426514] [G loss: 0.8746218085289001] [ID loss: 0.1398422122001648] [LR: 0.0002]\n",
            "[Epoch 141/1000] [Batch 2400] [D loss f: 0.31949734687805176 r: 0.4248898923397064] [G loss: 0.8257043361663818] [ID loss: 0.14180977642536163] [LR: 0.0002]\n",
            "[Epoch 141/1000] [Batch 3000] [D loss f: 0.3156767785549164 r: 0.42497894167900085] [G loss: 0.8487580418586731] [ID loss: 0.14000838994979858] [LR: 0.0002]\n",
            "Time/Batch 0.11269937663271734\n",
            "Memory {'current': 679667456, 'peak': 5224965120}\n",
            "Mean D loss: 0.31840530037879944 Mean G loss: 0.8527068495750427 Mean ID loss: 0.1409512311220169\n",
            "[Epoch 142/1000] [Batch 0] [D loss f: 0.3185498118400574 r: 0.42272719740867615] [G loss: 0.8665533065795898] [ID loss: 0.14228469133377075] [LR: 0.0002]\n",
            "[Epoch 142/1000] [Batch 600] [D loss f: 0.3111267685890198 r: 0.4129331409931183] [G loss: 0.8655523657798767] [ID loss: 0.14077284932136536] [LR: 0.0002]\n",
            "[Epoch 142/1000] [Batch 1200] [D loss f: 0.3193328380584717 r: 0.4230474829673767] [G loss: 0.8441410660743713] [ID loss: 0.14089329540729523] [LR: 0.0002]\n",
            "[Epoch 142/1000] [Batch 1800] [D loss f: 0.31146007776260376 r: 0.42979028820991516] [G loss: 0.8765464425086975] [ID loss: 0.14086708426475525] [LR: 0.0002]\n",
            "[Epoch 142/1000] [Batch 2400] [D loss f: 0.3114270567893982 r: 0.41993385553359985] [G loss: 0.836871862411499] [ID loss: 0.14170846343040466] [LR: 0.0002]\n",
            "[Epoch 142/1000] [Batch 3000] [D loss f: 0.3185865879058838 r: 0.4239742159843445] [G loss: 0.8292424082756042] [ID loss: 0.14109905064105988] [LR: 0.0002]\n",
            "Time/Batch 0.11255433985515596\n",
            "Memory {'current': 681992192, 'peak': 5224965120}\n",
            "Mean D loss: 0.3151078522205353 Mean G loss: 0.8487640023231506 Mean ID loss: 0.1411244124174118\n",
            "[Epoch 143/1000] [Batch 0] [D loss f: 0.3199891448020935 r: 0.4147811532020569] [G loss: 0.8366800546646118] [ID loss: 0.1415395736694336] [LR: 0.0002]\n",
            "[Epoch 143/1000] [Batch 600] [D loss f: 0.31316158175468445 r: 0.42528191208839417] [G loss: 0.843381941318512] [ID loss: 0.14162027835845947] [LR: 0.0002]\n",
            "[Epoch 143/1000] [Batch 1200] [D loss f: 0.31654855608940125 r: 0.42624133825302124] [G loss: 0.8485410809516907] [ID loss: 0.14091624319553375] [LR: 0.0002]\n",
            "[Epoch 143/1000] [Batch 1800] [D loss f: 0.31353357434272766 r: 0.42215046286582947] [G loss: 0.8329076170921326] [ID loss: 0.14012642204761505] [LR: 0.0002]\n",
            "[Epoch 143/1000] [Batch 2400] [D loss f: 0.31809544563293457 r: 0.4258638620376587] [G loss: 0.8526716828346252] [ID loss: 0.14027588069438934] [LR: 0.0002]\n",
            "[Epoch 143/1000] [Batch 3000] [D loss f: 0.31716975569725037 r: 0.42099207639694214] [G loss: 0.8679109811782837] [ID loss: 0.13985447585582733] [LR: 0.0002]\n",
            "Time/Batch 0.11243255673822833\n",
            "Memory {'current': 684315392, 'peak': 5224965120}\n",
            "Mean D loss: 0.31603118777275085 Mean G loss: 0.8491753935813904 Mean ID loss: 0.14061549305915833\n",
            "[Epoch 144/1000] [Batch 0] [D loss f: 0.3189401626586914 r: 0.42023953795433044] [G loss: 0.8492296934127808] [ID loss: 0.14103375375270844] [LR: 0.0002]\n",
            "[Epoch 144/1000] [Batch 600] [D loss f: 0.3163999021053314 r: 0.4182634949684143] [G loss: 0.8416672945022583] [ID loss: 0.13933947682380676] [LR: 0.0002]\n",
            "[Epoch 144/1000] [Batch 1200] [D loss f: 0.3029082119464874 r: 0.4203854501247406] [G loss: 0.8284533619880676] [ID loss: 0.1399952918291092] [LR: 0.0002]\n",
            "[Epoch 144/1000] [Batch 1800] [D loss f: 0.3187113404273987 r: 0.4149322807788849] [G loss: 0.8338762521743774] [ID loss: 0.1388702243566513] [LR: 0.0002]\n",
            "[Epoch 144/1000] [Batch 2400] [D loss f: 0.31138184666633606 r: 0.4194895327091217] [G loss: 0.8408618569374084] [ID loss: 0.13840100169181824] [LR: 0.0002]\n",
            "[Epoch 144/1000] [Batch 3000] [D loss f: 0.3203188478946686 r: 0.4192027747631073] [G loss: 0.8446685671806335] [ID loss: 0.13909292221069336] [LR: 0.0002]\n",
            "Time/Batch 0.11245346447778126\n",
            "Memory {'current': 686637568, 'peak': 5224965120}\n",
            "Mean D loss: 0.3138609528541565 Mean G loss: 0.8398074507713318 Mean ID loss: 0.1392722725868225\n",
            "[Epoch 145/1000] [Batch 0] [D loss f: 0.31282562017440796 r: 0.4109013080596924] [G loss: 0.8544573783874512] [ID loss: 0.1402689814567566] [LR: 0.0002]\n",
            "[Epoch 145/1000] [Batch 600] [D loss f: 0.31738898158073425 r: 0.4159911572933197] [G loss: 0.8427072167396545] [ID loss: 0.13869281113147736] [LR: 0.0002]\n",
            "[Epoch 145/1000] [Batch 1200] [D loss f: 0.3126277029514313 r: 0.4250793755054474] [G loss: 0.8399190306663513] [ID loss: 0.13972407579421997] [LR: 0.0002]\n",
            "[Epoch 145/1000] [Batch 1800] [D loss f: 0.3050752878189087 r: 0.4160970151424408] [G loss: 0.8539980053901672] [ID loss: 0.13874076306819916] [LR: 0.0002]\n",
            "[Epoch 145/1000] [Batch 2400] [D loss f: 0.3122807741165161 r: 0.41887784004211426] [G loss: 0.871734619140625] [ID loss: 0.13852861523628235] [LR: 0.0002]\n",
            "[Epoch 145/1000] [Batch 3000] [D loss f: 0.30939924716949463 r: 0.41798338294029236] [G loss: 0.8423746824264526] [ID loss: 0.1378515511751175] [LR: 0.0002]\n",
            "Time/Batch 0.11262405191990013\n",
            "Memory {'current': 688962048, 'peak': 5224965120}\n",
            "Saving...\n",
            "Mean D loss: 0.3104579746723175 Mean G loss: 0.8498249053955078 Mean ID loss: 0.13885076344013214\n",
            "[Epoch 146/1000] [Batch 0] [D loss f: 0.3039558231830597 r: 0.40959563851356506] [G loss: 0.8474273085594177] [ID loss: 0.13987737894058228] [LR: 0.0002]\n",
            "[Epoch 146/1000] [Batch 600] [D loss f: 0.3145691752433777 r: 0.4083804786205292] [G loss: 0.8479284048080444] [ID loss: 0.1390921175479889] [LR: 0.0002]\n",
            "[Epoch 146/1000] [Batch 1200] [D loss f: 0.31890633702278137 r: 0.41807395219802856] [G loss: 0.829667329788208] [ID loss: 0.14029547572135925] [LR: 0.0002]\n",
            "[Epoch 146/1000] [Batch 1800] [D loss f: 0.3080042898654938 r: 0.41700106859207153] [G loss: 0.8308557868003845] [ID loss: 0.1400987058877945] [LR: 0.0002]\n",
            "[Epoch 146/1000] [Batch 2400] [D loss f: 0.3085726499557495 r: 0.4093324840068817] [G loss: 0.8556295037269592] [ID loss: 0.13797353208065033] [LR: 0.0002]\n",
            "[Epoch 146/1000] [Batch 3000] [D loss f: 0.30696219205856323 r: 0.42035993933677673] [G loss: 0.8432860970497131] [ID loss: 0.1394709199666977] [LR: 0.0002]\n",
            "Time/Batch 0.11254381705984377\n",
            "Memory {'current': 691284736, 'peak': 5224965120}\n",
            "Mean D loss: 0.31057611107826233 Mean G loss: 0.8415272831916809 Mean ID loss: 0.13930325210094452\n",
            "[Epoch 147/1000] [Batch 0] [D loss f: 0.30516117811203003 r: 0.4131363332271576] [G loss: 0.8408942222595215] [ID loss: 0.1387346386909485] [LR: 0.0002]\n",
            "[Epoch 147/1000] [Batch 600] [D loss f: 0.30287227034568787 r: 0.41287729144096375] [G loss: 0.8525031208992004] [ID loss: 0.1385495811700821] [LR: 0.0002]\n",
            "[Epoch 147/1000] [Batch 1200] [D loss f: 0.30269181728363037 r: 0.40434616804122925] [G loss: 0.8363938927650452] [ID loss: 0.13858115673065186] [LR: 0.0002]\n",
            "[Epoch 147/1000] [Batch 1800] [D loss f: 0.3053247630596161 r: 0.41155192255973816] [G loss: 0.8551104664802551] [ID loss: 0.13952606916427612] [LR: 0.0002]\n",
            "[Epoch 147/1000] [Batch 2400] [D loss f: 0.3009944260120392 r: 0.4083717465400696] [G loss: 0.8541891574859619] [ID loss: 0.13835695385932922] [LR: 0.0002]\n",
            "[Epoch 147/1000] [Batch 3000] [D loss f: 0.304085910320282 r: 0.4036605954170227] [G loss: 0.8757653832435608] [ID loss: 0.13776011765003204] [LR: 0.0002]\n",
            "Time/Batch 0.11261644481140891\n",
            "Memory {'current': 693608704, 'peak': 5224965120}\n",
            "Mean D loss: 0.30415078997612 Mean G loss: 0.855518102645874 Mean ID loss: 0.13851791620254517\n",
            "[Epoch 148/1000] [Batch 0] [D loss f: 0.3106209635734558 r: 0.40855327248573303] [G loss: 0.8616734147071838] [ID loss: 0.13822494447231293] [LR: 0.0002]\n",
            "[Epoch 148/1000] [Batch 600] [D loss f: 0.31231236457824707 r: 0.39644917845726013] [G loss: 0.8252409100532532] [ID loss: 0.13879503309726715] [LR: 0.0002]\n",
            "[Epoch 148/1000] [Batch 1200] [D loss f: 0.31377264857292175 r: 0.4075205326080322] [G loss: 0.8445814251899719] [ID loss: 0.138011172413826] [LR: 0.0002]\n",
            "[Epoch 148/1000] [Batch 1800] [D loss f: 0.3109148442745209 r: 0.41151002049446106] [G loss: 0.8550010919570923] [ID loss: 0.13799509406089783] [LR: 0.0002]\n",
            "[Epoch 148/1000] [Batch 2400] [D loss f: 0.3059544563293457 r: 0.4046902358531952] [G loss: 0.8442795872688293] [ID loss: 0.1377715766429901] [LR: 0.0002]\n",
            "[Epoch 148/1000] [Batch 3000] [D loss f: 0.3107799291610718 r: 0.4016374945640564] [G loss: 0.8582637310028076] [ID loss: 0.13746416568756104] [LR: 0.0002]\n",
            "Time/Batch 0.11252044313869218\n",
            "Memory {'current': 695933952, 'peak': 5224965120}\n",
            "Mean D loss: 0.31037166714668274 Mean G loss: 0.8465169072151184 Mean ID loss: 0.13805797696113586\n",
            "[Epoch 149/1000] [Batch 0] [D loss f: 0.30724072456359863 r: 0.40582382678985596] [G loss: 0.8548094630241394] [ID loss: 0.1384333074092865] [LR: 0.0002]\n",
            "[Epoch 149/1000] [Batch 600] [D loss f: 0.3189125061035156 r: 0.3969854712486267] [G loss: 0.8048207759857178] [ID loss: 0.1378069818019867] [LR: 0.0002]\n",
            "[Epoch 149/1000] [Batch 1200] [D loss f: 0.3102937340736389 r: 0.4074435830116272] [G loss: 0.8665682077407837] [ID loss: 0.1386733204126358] [LR: 0.0002]\n",
            "[Epoch 149/1000] [Batch 1800] [D loss f: 0.3089146912097931 r: 0.4018295705318451] [G loss: 0.8683804273605347] [ID loss: 0.1372056007385254] [LR: 0.0002]\n",
            "[Epoch 149/1000] [Batch 2400] [D loss f: 0.3057310879230499 r: 0.40376928448677063] [G loss: 0.8263310790061951] [ID loss: 0.1387198567390442] [LR: 0.0002]\n",
            "[Epoch 149/1000] [Batch 3000] [D loss f: 0.30976271629333496 r: 0.4058927595615387] [G loss: 0.8567777276039124] [ID loss: 0.13631726801395416] [LR: 0.0002]\n",
            "Time/Batch 0.11247352934248092\n",
            "Memory {'current': 698256640, 'peak': 5224965120}\n",
            "Mean D loss: 0.3097650110721588 Mean G loss: 0.8453420996665955 Mean ID loss: 0.137760192155838\n",
            "[Epoch 150/1000] [Batch 0] [D loss f: 0.30287808179855347 r: 0.4041208028793335] [G loss: 0.8507800698280334] [ID loss: 0.13789105415344238] [LR: 0.0002]\n",
            "[Epoch 150/1000] [Batch 600] [D loss f: 0.30531784892082214 r: 0.4016108810901642] [G loss: 0.8497401475906372] [ID loss: 0.13869531452655792] [LR: 0.0002]\n",
            "[Epoch 150/1000] [Batch 1200] [D loss f: 0.30863073468208313 r: 0.4030419886112213] [G loss: 0.834109365940094] [ID loss: 0.13810275495052338] [LR: 0.0002]\n",
            "[Epoch 150/1000] [Batch 1800] [D loss f: 0.3039987087249756 r: 0.40340518951416016] [G loss: 0.8612325191497803] [ID loss: 0.13894861936569214] [LR: 0.0002]\n",
            "[Epoch 150/1000] [Batch 2400] [D loss f: 0.3077601492404938 r: 0.4093386232852936] [G loss: 0.8238716721534729] [ID loss: 0.1368218958377838] [LR: 0.0002]\n",
            "[Epoch 150/1000] [Batch 3000] [D loss f: 0.3030051290988922 r: 0.39948925375938416] [G loss: 0.8709903955459595] [ID loss: 0.1383616030216217] [LR: 0.0002]\n",
            "Time/Batch 0.1123960146407812\n",
            "Memory {'current': 700578816, 'peak': 5224965120}\n",
            "Saving...\n",
            "Mean D loss: 0.30578359961509705 Mean G loss: 0.8487616181373596 Mean ID loss: 0.13826102018356323\n",
            "[Epoch 151/1000] [Batch 0] [D loss f: 0.3059893548488617 r: 0.39931541681289673] [G loss: 0.8548550605773926] [ID loss: 0.13883323967456818] [LR: 0.0002]\n",
            "[Epoch 151/1000] [Batch 600] [D loss f: 0.3046250641345978 r: 0.4054557681083679] [G loss: 0.8646808862686157] [ID loss: 0.13686995208263397] [LR: 0.0002]\n",
            "[Epoch 151/1000] [Batch 1200] [D loss f: 0.3064168691635132 r: 0.3936631977558136] [G loss: 0.8635602593421936] [ID loss: 0.13767613470554352] [LR: 0.0002]\n",
            "[Epoch 151/1000] [Batch 1800] [D loss f: 0.3091590404510498 r: 0.4055275619029999] [G loss: 0.8540583252906799] [ID loss: 0.13671238720417023] [LR: 0.0002]\n",
            "[Epoch 151/1000] [Batch 2400] [D loss f: 0.2971652150154114 r: 0.3982595205307007] [G loss: 0.8454127311706543] [ID loss: 0.13741983473300934] [LR: 0.0002]\n",
            "[Epoch 151/1000] [Batch 3000] [D loss f: 0.30845341086387634 r: 0.401109516620636] [G loss: 0.8600457906723022] [ID loss: 0.13608017563819885] [LR: 0.0002]\n",
            "Time/Batch 0.11221041026219139\n",
            "Memory {'current': 702903296, 'peak': 5224965120}\n",
            "Mean D loss: 0.3047666549682617 Mean G loss: 0.8531068563461304 Mean ID loss: 0.13720642030239105\n",
            "[Epoch 152/1000] [Batch 0] [D loss f: 0.30168211460113525 r: 0.39578935503959656] [G loss: 0.819804847240448] [ID loss: 0.13905930519104004] [LR: 0.0002]\n",
            "[Epoch 152/1000] [Batch 600] [D loss f: 0.3121334910392761 r: 0.3977990746498108] [G loss: 0.8522140383720398] [ID loss: 0.1379387229681015] [LR: 0.0002]\n",
            "[Epoch 152/1000] [Batch 1200] [D loss f: 0.3080042004585266 r: 0.3926772177219391] [G loss: 0.8461408019065857] [ID loss: 0.13725118339061737] [LR: 0.0002]\n",
            "[Epoch 152/1000] [Batch 1800] [D loss f: 0.3018135726451874 r: 0.39000365138053894] [G loss: 0.8247396349906921] [ID loss: 0.13857309520244598] [LR: 0.0002]\n",
            "[Epoch 152/1000] [Batch 2400] [D loss f: 0.2992747128009796 r: 0.40301498770713806] [G loss: 0.8763322234153748] [ID loss: 0.13741381466388702] [LR: 0.0002]\n",
            "[Epoch 152/1000] [Batch 3000] [D loss f: 0.3065025210380554 r: 0.38770440220832825] [G loss: 0.8512670993804932] [ID loss: 0.13717497885227203] [LR: 0.0002]\n",
            "Time/Batch 0.1124283926827567\n",
            "Memory {'current': 705226752, 'peak': 5224965120}\n",
            "Mean D loss: 0.3049638569355011 Mean G loss: 0.8501724004745483 Mean ID loss: 0.13755729794502258\n",
            "[Epoch 153/1000] [Batch 0] [D loss f: 0.301328182220459 r: 0.3914390504360199] [G loss: 0.8495959043502808] [ID loss: 0.13671599328517914] [LR: 0.0002]\n",
            "[Epoch 153/1000] [Batch 600] [D loss f: 0.30463653802871704 r: 0.3967439532279968] [G loss: 0.852367639541626] [ID loss: 0.13773532211780548] [LR: 0.0002]\n",
            "[Epoch 153/1000] [Batch 1200] [D loss f: 0.31083381175994873 r: 0.39786043763160706] [G loss: 0.8356674313545227] [ID loss: 0.1381663978099823] [LR: 0.0002]\n",
            "[Epoch 153/1000] [Batch 1800] [D loss f: 0.3091358542442322 r: 0.39811015129089355] [G loss: 0.8313869833946228] [ID loss: 0.13679519295692444] [LR: 0.0002]\n",
            "[Epoch 153/1000] [Batch 2400] [D loss f: 0.3030143082141876 r: 0.3930245637893677] [G loss: 0.8497264385223389] [ID loss: 0.13662229478359222] [LR: 0.0002]\n",
            "[Epoch 153/1000] [Batch 3000] [D loss f: 0.2943301498889923 r: 0.39380908012390137] [G loss: 0.8305713534355164] [ID loss: 0.1367792785167694] [LR: 0.0002]\n",
            "Time/Batch 0.11244168903882332\n",
            "Memory {'current': 707549184, 'peak': 5224965120}\n",
            "Mean D loss: 0.30433401465415955 Mean G loss: 0.8440907597541809 Mean ID loss: 0.13710492849349976\n",
            "[Epoch 154/1000] [Batch 0] [D loss f: 0.3043776750564575 r: 0.3944665789604187] [G loss: 0.8746063113212585] [ID loss: 0.13626325130462646] [LR: 0.0002]\n",
            "[Epoch 154/1000] [Batch 600] [D loss f: 0.3062697649002075 r: 0.38629817962646484] [G loss: 0.8319312334060669] [ID loss: 0.1366395503282547] [LR: 0.0002]\n",
            "[Epoch 154/1000] [Batch 1200] [D loss f: 0.3020533323287964 r: 0.4015059173107147] [G loss: 0.8408966064453125] [ID loss: 0.13624952733516693] [LR: 0.0002]\n",
            "[Epoch 154/1000] [Batch 1800] [D loss f: 0.30221742391586304 r: 0.38867273926734924] [G loss: 0.8456149697303772] [ID loss: 0.1367759257555008] [LR: 0.0002]\n",
            "[Epoch 154/1000] [Batch 2400] [D loss f: 0.3032999634742737 r: 0.3874405324459076] [G loss: 0.833853542804718] [ID loss: 0.13564620912075043] [LR: 0.0002]\n",
            "[Epoch 154/1000] [Batch 3000] [D loss f: 0.29886767268180847 r: 0.3913312256336212] [G loss: 0.8522195219993591] [ID loss: 0.13559062778949738] [LR: 0.0002]\n",
            "Time/Batch 0.11238907660406383\n",
            "Memory {'current': 709875712, 'peak': 5224965120}\n",
            "Mean D loss: 0.3015681207180023 Mean G loss: 0.8435468077659607 Mean ID loss: 0.1362885981798172\n",
            "[Epoch 155/1000] [Batch 0] [D loss f: 0.29372745752334595 r: 0.3840254843235016] [G loss: 0.8637564182281494] [ID loss: 0.1371353417634964] [LR: 0.0002]\n",
            "[Epoch 155/1000] [Batch 600] [D loss f: 0.31001177430152893 r: 0.3943653702735901] [G loss: 0.8372789025306702] [ID loss: 0.13515980541706085] [LR: 0.0002]\n",
            "[Epoch 155/1000] [Batch 1200] [D loss f: 0.3048045337200165 r: 0.39582565426826477] [G loss: 0.820862352848053] [ID loss: 0.13480594754219055] [LR: 0.0002]\n",
            "[Epoch 155/1000] [Batch 1800] [D loss f: 0.2977135479450226 r: 0.3957660496234894] [G loss: 0.873273491859436] [ID loss: 0.13620144128799438] [LR: 0.0002]\n",
            "[Epoch 155/1000] [Batch 2400] [D loss f: 0.3019897937774658 r: 0.39664143323898315] [G loss: 0.8389586210250854] [ID loss: 0.13650661706924438] [LR: 0.0002]\n",
            "[Epoch 155/1000] [Batch 3000] [D loss f: 0.2990836203098297 r: 0.3937304615974426] [G loss: 0.839599072933197] [ID loss: 0.13661453127861023] [LR: 0.0002]\n",
            "Time/Batch 0.11245337701558646\n",
            "Memory {'current': 712194816, 'peak': 5224965120}\n",
            "Saving...\n",
            "Mean D loss: 0.3022913336753845 Mean G loss: 0.8414587378501892 Mean ID loss: 0.13575732707977295\n",
            "[Epoch 156/1000] [Batch 0] [D loss f: 0.2983090281486511 r: 0.38495051860809326] [G loss: 0.8387191891670227] [ID loss: 0.13497722148895264] [LR: 0.0002]\n",
            "[Epoch 156/1000] [Batch 600] [D loss f: 0.3023029565811157 r: 0.38915151357650757] [G loss: 0.876023530960083] [ID loss: 0.1362234503030777] [LR: 0.0002]\n",
            "[Epoch 156/1000] [Batch 1200] [D loss f: 0.3044847548007965 r: 0.38318222761154175] [G loss: 0.8385576605796814] [ID loss: 0.13552039861679077] [LR: 0.0002]\n",
            "[Epoch 156/1000] [Batch 1800] [D loss f: 0.29873326420783997 r: 0.3913051187992096] [G loss: 0.8505970239639282] [ID loss: 0.13640782237052917] [LR: 0.0002]\n",
            "[Epoch 156/1000] [Batch 2400] [D loss f: 0.30930349230766296 r: 0.3907759189605713] [G loss: 0.8565928339958191] [ID loss: 0.13674606382846832] [LR: 0.0002]\n",
            "[Epoch 156/1000] [Batch 3000] [D loss f: 0.2984786331653595 r: 0.38660889863967896] [G loss: 0.8363659381866455] [ID loss: 0.1378275752067566] [LR: 0.0002]\n",
            "Time/Batch 0.11260355878478986\n",
            "Memory {'current': 714519808, 'peak': 5224965120}\n",
            "Mean D loss: 0.30209437012672424 Mean G loss: 0.8540194034576416 Mean ID loss: 0.13665668666362762\n",
            "[Epoch 157/1000] [Batch 0] [D loss f: 0.2986975610256195 r: 0.38667434453964233] [G loss: 0.8704976439476013] [ID loss: 0.1375071108341217] [LR: 0.0002]\n",
            "[Epoch 157/1000] [Batch 600] [D loss f: 0.2972885072231293 r: 0.39385002851486206] [G loss: 0.8315544724464417] [ID loss: 0.1374015361070633] [LR: 0.0002]\n",
            "[Epoch 157/1000] [Batch 1200] [D loss f: 0.306613951921463 r: 0.3841756284236908] [G loss: 0.845564067363739] [ID loss: 0.1352585405111313] [LR: 0.0002]\n",
            "[Epoch 157/1000] [Batch 1800] [D loss f: 0.30130767822265625 r: 0.3919011056423187] [G loss: 0.8653989434242249] [ID loss: 0.13615360856056213] [LR: 0.0002]\n",
            "[Epoch 157/1000] [Batch 2400] [D loss f: 0.3063918352127075 r: 0.37843599915504456] [G loss: 0.836452841758728] [ID loss: 0.1363079845905304] [LR: 0.0002]\n",
            "[Epoch 157/1000] [Batch 3000] [D loss f: 0.30441713333129883 r: 0.378605455160141] [G loss: 0.8351337909698486] [ID loss: 0.13593921065330505] [LR: 0.0002]\n",
            "Time/Batch 0.11255018533082095\n",
            "Memory {'current': 716842752, 'peak': 5224965120}\n",
            "Mean D loss: 0.3027169704437256 Mean G loss: 0.8430523872375488 Mean ID loss: 0.13632160425186157\n",
            "[Epoch 158/1000] [Batch 0] [D loss f: 0.2990340292453766 r: 0.3888061046600342] [G loss: 0.8449150323867798] [ID loss: 0.13711322844028473] [LR: 0.0002]\n",
            "[Epoch 158/1000] [Batch 600] [D loss f: 0.3053208291530609 r: 0.3909452557563782] [G loss: 0.8665529489517212] [ID loss: 0.1340821534395218] [LR: 0.0002]\n",
            "[Epoch 158/1000] [Batch 1200] [D loss f: 0.29879429936408997 r: 0.38623982667922974] [G loss: 0.844898521900177] [ID loss: 0.13637685775756836] [LR: 0.0002]\n",
            "[Epoch 158/1000] [Batch 1800] [D loss f: 0.3041200041770935 r: 0.39037248492240906] [G loss: 0.8369850516319275] [ID loss: 0.13572289049625397] [LR: 0.0002]\n",
            "[Epoch 158/1000] [Batch 2400] [D loss f: 0.2998562753200531 r: 0.3831412196159363] [G loss: 0.8702513575553894] [ID loss: 0.13549429178237915] [LR: 0.0002]\n",
            "[Epoch 158/1000] [Batch 3000] [D loss f: 0.3040509819984436 r: 0.37892740964889526] [G loss: 0.8254812955856323] [ID loss: 0.1370628923177719] [LR: 0.0002]\n",
            "Time/Batch 0.11262523966771865\n",
            "Memory {'current': 719167744, 'peak': 5224965120}\n",
            "Mean D loss: 0.30190309882164 Mean G loss: 0.8494620323181152 Mean ID loss: 0.13566535711288452\n",
            "[Epoch 159/1000] [Batch 0] [D loss f: 0.29750654101371765 r: 0.39338964223861694] [G loss: 0.8546671271324158] [ID loss: 0.13507704436779022] [LR: 0.0002]\n",
            "[Epoch 159/1000] [Batch 600] [D loss f: 0.2961236238479614 r: 0.3866158425807953] [G loss: 0.8468218445777893] [ID loss: 0.13564521074295044] [LR: 0.0002]\n",
            "[Epoch 159/1000] [Batch 1200] [D loss f: 0.3011399805545807 r: 0.3788987696170807] [G loss: 0.8474575877189636] [ID loss: 0.13563095033168793] [LR: 0.0002]\n",
            "[Epoch 159/1000] [Batch 1800] [D loss f: 0.30458688735961914 r: 0.38174688816070557] [G loss: 0.8205682635307312] [ID loss: 0.13567599654197693] [LR: 0.0002]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-f6nSiF95H-"
      },
      "source": [
        "tf.config.experimental.get_memory_info('GPU:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WnStxI1g1XVx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}